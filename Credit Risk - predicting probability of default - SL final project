<font face="verdana" color="purple" size = 5>
PROJECT INTRODUCTION
</font>

<font face="verdana" color="purple" size = 2>
    In this project, using dataset public in HomeCredit competition for predictive default loan in credit risk. To understand better the background of this project, let's walk through this company profile and why they need to predict probability of default.
    <br>
    <br>
    Home Credit is a global consumer finance provider focusing on responsible lending to underbanked populations, leveraging technology for access to financial services. In other to reduce credit loss where customers are not able to pay back the loan, one of the methods is assessing customer's probability of default before granting certain loan amount or credit limit, via authorized data on the applicant. In this scope of project, will focus on Cash Loan product, which is to predict probability of default on applicant applying for a certain loan amount.
    <br>
    <br>
    For this problem of classification on whether or not customer will be default, supervised learning algorithm will be used with target to build main model using Logistic Regression. As requested in this scope of competition, AUC metric will be used to evaluate model's performance, with aim to achieve minimum 0.68. Additionally, AIC metric will also be used to measure model's goodness and complexity, in order to select most efficient model.
    <br>
    <br>
    For feature selection criteria, IV and WoE will be used to measure feature's importance, as well as p-value for feature significant.
    <br>
    <br>
    Source of data reference: <br>
    HOME CREDIT GROUP · FEATURED PREDICTION COMPETITION (2018)  · 
    https://www.kaggle.com/competitions/home-credit-default-risk/data
    
</font>

#import necessary tools
import pandas as pd
import numpy as np
pd.options.display.float_format = '{:.2f}'.format
pd.options.mode.chained_assignment = None
from sklearn.impute import SimpleImputer
import seaborn as sns
import matplotlib.pyplot as plt

#import data
path_desc = '/11. Machine Learning/11.1 Intro to ML Supervised Learning/final project tips/dataset for project/HC data/HomeCredit_columns_description.csv'
path_apptrain = '/11. Machine Learning/11.1 Intro to ML Supervised Learning/final project tips/dataset for project/HC data/application_train.csv'
path_apptest = '/11. Machine Learning/11.1 Intro to ML Supervised Learning/final project tips/dataset for project/HC data/application_test.csv'

data_desc = pd.read_csv(path_desc, encoding='cp1252')
data_apptrain = pd.read_csv(path_apptrain, encoding='cp1252')
data_apptest = pd.read_csv(path_apptest, encoding='cp1252')


## 1. DATA EXPLORATION <br> 


data_apptrain.shape

As the original dataset contain 307511 rows and 122 columns, including both Cash Loans and Revolving Loans. In this scope of project, filter to use dataset of only cash loan product:


df = data_apptrain[data_apptrain['NAME_CONTRACT_TYPE']== 'Cash loans']
df.shape

Remaining size of Cash Loans dataset is 278232 rows x 122 collumns will be used for data exploratory and model training and assessment onwards.
For cross check and identify proper method during data cleaning process, data_apptest on Cash Loans with 48305 rows x 121 cols will also be used to double check of logic, but will not participate to model training.

## 1.1 Inspection on Cash Loan dataset
    


In this section, we will inspect data and focus on below step:
    
    1. handling missing data;
    2. handling incorrect datatype;
    3. impute data inconsistency.
    

#check datatype of data_trainCL
set_dtype = set()
for i in range(len(df.columns)):
    set_dtype.add(df.iloc[:,i].dtypes)
    
print(set_dtype)

#inspecting missing values and number of values
def missing_inspection(df):
    numeric_missing = {}
    cat_missing = {}
    
    for i in range(len(df.columns)):
        if df.iloc[:,i].dtypes == 'int64' or df.iloc[:,i].dtypes == 'float64':
            print (f'missing value in column {df.columns[i]} is {df.iloc[:,i].isnull().sum()/len(df)} \n')
            if df.iloc[:,i].isnull().sum() > 0:
                numeric_missing[df.columns[i]]=df.iloc[:,i].isnull().sum()
        else:
            print(f'number of missing values in this column is {df.iloc[:,i].isnull().sum()/len(df)} \n{df.iloc[:,i].value_counts(dropna = False)} \n')
            if df.iloc[:,i].isnull().sum() > 0:
                cat_missing[df.columns[i]]=df.iloc[:,i].isnull().sum()
    return print(f'Summary of missing data: {len(numeric_missing.keys())+len(cat_missing.keys())} cols has missing value, in which Numeric is {len(numeric_missing.keys())} cols and Category is {len(cat_missing.keys())} cols \n \n NUMERIC: \n {numeric_missing}, \n \n CATEGORY: \n {cat_missing}')

#checking characters of missing value in data_apptest
missing_inspection(data_apptest[data_apptest['NAME_CONTRACT_TYPE']=='Cash loans'])

#comparing to characters of missing value in training set
missing_inspection(df)


Base on data description provided by source, as well as crosschecking on testset, together with above summary of missing value and data inconsistency, removing NA or imputation will process as following: <br>
- inspect on 1 data missing on _SOCIAL_CIRCLE and DAYS_LAST_PHONE_CHANGE, to remove if it is technical issue in data gathering by the source (not appear in testset), otherwise impute to appropriate value. <br>
- for normalized data: missing value will be replace by -1 <br>
- for days-to-extract-time data: > 0 is inconsistent data. If x > 0 or missing, impute by 1 <br>
- for continuous data of currency amount: impute by mean value. <br>
- for continuous data of other type: inspect data detail to consider mean or -1 value to be applied. <br>
- for category data: denote missing value group to "unknown"


## 1.2 processing on single data missing

#associated on 1 record, temporary to impute by -1, later when perform data analysis, will consider to update this into proper value
df['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(-1,inplace = True)
df['DEF_30_CNT_SOCIAL_CIRCLE'].fillna(-1,inplace = True)
df['OBS_60_CNT_SOCIAL_CIRCLE'].fillna(-1,inplace = True)
df['DEF_60_CNT_SOCIAL_CIRCLE'].fillna(-1,inplace = True)

#technical issue, to drop this record
df.dropna(subset=['DAYS_LAST_PHONE_CHANGE'], inplace=True)

## 1.3 processing on numeric data: normalized and days

numeric_missing = []
cat_missing = []
    
for i in range(len(df.columns)):
    if df.iloc[:,i].dtypes == 'int64' or df.iloc[:,i].dtypes == 'float64':
        if df.iloc[:,i].isnull().sum() > 0:
            numeric_missing.append(df.columns[i])
    else:
        if df.iloc[:,i].isnull().sum() > 0:
            cat_missing.append(df.columns[i])

#impute numeric value: missing value
for col in numeric_missing:
    if col in data_desc[data_desc['Special'].isin(['normalized', 'normalized '])]['Row'].tolist():
        df[col].fillna(-1,inplace=True) 
    elif col in data_desc[data_desc['Special']=='time only relative to the application']['Row'].tolist():
        df[col].fillna(1,inplace = True)

#impute numeric value: inconsistent value
#days count to application time should be less than 0 (past data), thus impute to 1 to remark invalid data to same group of missing data
for col in numeric_missing:
    if col in data_desc[data_desc['Special']=='time only relative to the application']['Row'].tolist() and len(df.loc[df[col] > 0]) > 0:
        df[col] = df[col].apply(lambda x: x if x < 0 else 1)

## 1.4 processing on continuous data of currency amount

let's have a look on 12 missing record of AMT_ANNUITY:

df[df['AMT_ANNUITY'].isnull()]

as Amount of Annuity is mandatory data, this missing can be technical issue during data gathering by the source, to impute by mean value.

df['AMT_ANNUITY'].fillna(df['AMT_ANNUITY'].mean(),inplace = True)

## 1.5 processing on continuous data of other type

inspect data of OWN_CAR_AGE to find proper impute value

df['OWN_CAR_AGE'].describe()

OWN_CAR_AGE has large missing value, presenting age or the own car, the larger of value the older of car, therefore the missing value will be denoted -1 to distinguish with 0 as new car.

df['OWN_CAR_AGE'].fillna(-1,inplace = True)

on remaining numeric data of AMT_REQ_CREDIT_BUREAU_time, missing data means there is no information, which is different from requesting data and showing 0 early request. Therefore, missing data of these fields will be denoted as -1

df['AMT_REQ_CREDIT_BUREAU_HOUR'].fillna(-1,inplace = True)
df['AMT_REQ_CREDIT_BUREAU_DAY'].fillna(-1,inplace = True)
df['AMT_REQ_CREDIT_BUREAU_WEEK'].fillna(-1,inplace = True)
df['AMT_REQ_CREDIT_BUREAU_MON'].fillna(-1,inplace = True)
df['AMT_REQ_CREDIT_BUREAU_QRT'].fillna(-1,inplace = True)
df['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(-1,inplace = True)

## 1.6 processing on category data

for cat in cat_missing:
    df[cat].fillna('unknown', inplace = True)

for i in cat_missing:
    print(df[i].value_counts())
    print('\n')

## 1.7 transform boolean value from category to numeric datatype.

#before transform:

boolean_list = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'EMERGENCYSTATE_MODE']
for b in boolean_list:
    print(df[b].value_counts())
    print('\n')

#transform:

df['CODE_GENDER'] = df['CODE_GENDER'].map({'F':1,'M':0})
df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].map({'Y':1,'N':0})
df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].map({'Y':1,'N':0})
df['EMERGENCYSTATE_MODE'] = df['EMERGENCYSTATE_MODE'].map({'Yes':1,'No':0, 'unknown': -1})

#after transform:

for b in boolean_list:
    print(df[b].value_counts())
    print('\n')

### last look on missing data before moving to data analysis part

missing_inspection(df)

all clean, now let's move to more interesting part: Data statistic and visualization. After this step, single data will be well understood and proper preparation for data transformation.

## 2. EXPLORATORY DATA ANALYSIS (EDA)


In this section, we will explore on statistical aspects of data, generally cover below step:

    1. Data Visualization;
    2. Summary Statistics; 
    3. Correlation Analysis; 
    4. Distribution Analysis.
Beforehand, let's build some function that will be used repeatedly:


#create neccessary plot for data visualization
# mainly use bar plot, histplot and plot_bar_target (when need to explore default rate association)

def plot_bar(df,column):
    plt.figure(figsize=(10,6))
    ax = sns.countplot(data=df, x=df[column],\
          palette=[sns.color_palette("pastel")[0]],order=df[column].value_counts().index)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
    plt.title('distribution of ' + column)
    plt.show() 
    plt.close()
    
def plot_hist(df, column):
    plt.figure(figsize=(10, 6))  # Set the figure size
    
    # Plot the histogram using Seaborn
    sns.histplot(df[column], color="skyblue", bins=30)
    
    # Customize the plot
    plt.title('Distribution of ' + column)  # Add title
    plt.xlabel(column)  # Add x-axis label
    plt.ylabel('Frequency')  # Add y-axis label
    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add grid for y-axis
    
    plt.show() 
    plt.close()
     
def plot_bar_target(df, column):
    df_data = df.groupby(column)['TARGET'].value_counts(normalize=True).unstack() * 100

    # Reset index to convert 'column' from index to column
    df_data = df_data.reset_index()

    # Create a bar plot using Seaborn
    fig, ax = plt.subplots(figsize=(10, 6)) 
    sns.barplot(x=column, y=0, color='skyblue', label='good', data=df_data)
    sns.barplot(x=column, y=1, color='lightcoral', label='bad', data=df_data)

    # Customize the plot
    plt.xlabel(column)
    plt.ylabel('%')
    plt.xticks(rotation=90)
    plt.title(column + ' by default rate')
    plt.legend(title='label')

    # Annotate the bars with percentages
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(f'{height:.1f}%', (p.get_x() + p.get_width() / 2., height),
                    ha='center', va='bottom', fontsize=10, color='black')

    plt.tight_layout()
    plt.show()
    plt.close()

cat_list = []
num_list = []
for i in df.columns.to_list():
    if (df[i].dtypes == 'int64' or df[i].dtypes == 'float64') and len(df[i].value_counts()) >= 20:
        num_list.append(i)
    else:
         cat_list.append(i)

### 2.1 Data Visualization & Summary Statistics

firstly let's have a look on category data

for i in cat_list[2:len(cat_list)]:
    plot_bar(df,i)

We can observe certain group has higher proportion in this dataset, let's have a look on how they associated with default rate

for i in cat_list[2:len(cat_list)]:
    plot_bar_target(df,i)

from above visualization, we can detect some following highights:
1. ORGANIZATION_TYPE has too many categories, will need to re-grouping into < 20 categories.
2. INCOME, EDUCATION, OCCUPATION and CHILDREN shows most differentiate of default rate amonsgt the categories.
3. INCOME_TYPE suggests most vulnerable group (high default rate) is on UNEMPLOYED & MATERNITY LEAVE. However, these groups account small proportion in training population.
4. EMPLOYED_PHONE presenting show higher default rate.
5. LIVING or WORKING address different than PERMANENT address show higher default rate. These groups account small proportion in training population.
6. for FLAG_DOCUMENT_x, sharing similar character as providing optional documents, consider to create new feature combining these data following logic: FLAG_EXTRA_DOCUMENT for FLAG_DOCUMENT_x = 1 then FLAG_EXTRA_DOCUMENT = 1 else 0.

#### update ORGANIZATION_TYPE & FLAG_DOCUMENT_x

#ORGANIZATION regrouping into smaller group at df['ORGANIZATION_TYPE_TRANSF']

def map_category_to_group(category):
    if category in ['Business Entity Type 1',
                    'Business Entity Type 2',
                    'Business Entity Type 3']:
        return 'Business Entity'
    
    elif category in ['Industry: type 1',
             'Industry: type 10',
             'Industry: type 11',
             'Industry: type 12',
             'Industry: type 13',
             'Industry: type 2',
             'Industry: type 3',
             'Industry: type 4',
             'Industry: type 5',
             'Industry: type 6',
             'Industry: type 7',
             'Industry: type 8',
             'Industry: type 9']:
        return 'Industry'
    
    elif category in ['Trade: type 1',
                 'Trade: type 2',
                 'Trade: type 3',
                 'Trade: type 4',
                 'Trade: type 5',
                 'Trade: type 6',
                 'Trade: type 7']:
        return 'Trade'
    
    elif category in ['Transport: type 1',
             'Transport: type 2',
             'Transport: type 3',
             'Transport: type 4']:
        return 'Transport'
    
    elif category in ['Bank','Insurance' ]:
        return 'Financial Service'
    
    elif category in ['Military','Police','Security Ministries', 'Government' ]:
        return 'Gov Officer'
    
    elif category in ['Hotel','Restaurant','Services', 'Cleaning']:
        return 'Hospitality Industry'
    
    elif category in ['Housing','Realtor']:
        return 'Real Estate'
    
    elif category in ['Telecom','Mobile', 'Postal']:
        return 'Telcomunicate'
    else:
        return category

df['ORGANIZATION_TYPE_TRANSF'] = df['ORGANIZATION_TYPE'].apply(map_category_to_group)

#grouping DOCUMENT_TYPE_x into FLAG_EXTRA_DOCUMENT
flag_doc = [col for col in df.columns if 'FLAG_DOCUMENT_' in col] #update documents list to look on default rate
flag_doc.remove('FLAG_DOCUMENT_3') #most of customer presenting this document, therefore we don't want to group it up.

#if customer provide one of document (value = 1), to classify value 1 into FLAG_DOCUMENT_EXTRA
df['FLAG_DOCUMENT_EXTRA'] = df[flag_doc].max(axis=1)

plot_bar(df, 'FLAG_DOCUMENT_EXTRA')

plot_bar_target(df, 'FLAG_DOCUMENT_EXTRA')

####  moving to numeric data, let's look on distribution of this group


for i in num_list[1:len(num_list)]:
    plot_hist(df, i)

we can observe following highlights with respective strategy:
1. anomaly data on DAYS_EMPLOYED, to inspect and update to invalid group if needed.
2. anomaly data on OWN_CAR_AGE after 60, to inspect and consider to group into bins.
3. normalize data to group into quartile bins.
4. remaining data to group into 10 bins.

After these refining steps, will have a look on association of grouping data vs. default rate.

#### inspect DAYS_EMPLOYED & OWN_CAR_AGE 

df[df['DAYS_EMPLOYED'] > 0]['DAYS_EMPLOYED'].describe()

As we observe there is only 1 value repeated on 52008 records, to update it to invalid group, denote by value 1.

df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].apply(lambda x: 1 if x > 0 else x)

Next, let's have a look on OWN_CAR_AGE

df[df['OWN_CAR_AGE'] > 60]['OWN_CAR_AGE'].describe()

We have 2766 records on different values, therefore let's group them into 1 group.

bins, bin_edges = pd.qcut(df['OWN_CAR_AGE'], q=10, duplicates='drop',retbins=True)
bin_edges = [-1,0,4,8,14,60,float('+inf')]
df['OWN_CAR_AGE_BIN'] = pd.cut(df['OWN_CAR_AGE'],bins = bin_edges, right=False)

plot_bar(df,'OWN_CAR_AGE_BIN')

plot_bar_target(df,'OWN_CAR_AGE_BIN')

from above plot, we can observe that customer has newer car, from 0 to 8 years, has lower default rate.

#### Group normalize data into quartile & invalid

#normalize data binning
for col in num_list[1:len(num_list)]:
    if col in data_desc[data_desc['Special'].isin(['normalized', 'normalized '])]['Row'].tolist():
        bin_edges = [-1,0,0.25,0.5,0.75,float('+inf')]
        new_col = col + '_BIN'
        df[new_col] = pd.cut(df[col],bins = bin_edges, right=False)


#days data binning
for col in num_list[1:len(num_list)]:
    if col in data_desc[data_desc['Special']=='time only relative to the application']['Row'].tolist() and len(df[df[col]==1]) > 0:
        bins, bin_edges = pd.qcut(df[col], q=10, duplicates='drop',retbins=True)
        bin_edges[0] = float('-inf')
        bin_edges[-2] = 1
        bin_edges[-1] = float('+inf')
        
        new_col = col + '_BIN'
        df[new_col] = pd.cut(df[col],bins = bin_edges, right=False)
        
    elif col in data_desc[data_desc['Special']=='time only relative to the application']['Row'].tolist() and len(df[df[col]==1]) == 0:
        bins, bin_edges = pd.qcut(df[col], q=10, duplicates='drop',retbins=True)
        bin_edges[0] = float('-inf')
        bin_edges[-1] = float('+inf')
        
        new_col = col + '_BIN'
        df[new_col] = pd.cut(df[col],bins = bin_edges, right=False)


#remaning continous data binning
for col in num_list[1:len(num_list)]:
    if col not in (data_desc[data_desc['Special'].isin(['normalized', 'normalized '])]['Row'].tolist() + data_desc[data_desc['Special']=='time only relative to the application']['Row'].tolist()):
        bins, bin_edges = pd.qcut(df[col], q=10, duplicates='drop', retbins=True)
        bin_edges[0] = float('-inf')
        bin_edges[-1] = float('+inf')

        new_col = col + '_BIN'
        df[new_col] = pd.cut(df[col],bins = bin_edges, right=False)

#exceptional binning on REGION_POPULATION_RELATIVE due to data skew:
bins, bin_edges = pd.qcut(df['REGION_POPULATION_RELATIVE'], q=5, duplicates='drop', retbins=True)
bin_edges[0] =  float('-inf')
df['REGION_POPULATION_RELATIVE_BIN'] = pd.cut(df['REGION_POPULATION_RELATIVE'],bins = bin_edges, right=False)
df['REGION_POPULATION_RELATIVE_BIN'].value_counts().sort_index(axis = 0)
        
bin_list = [col for col in df.columns if '_BIN' in col] #update bin list to look on default rate

Now let's have a look on the list of new bin to see association with default rate

for col in bin_list:
    plot_bar(df, col)

for col in bin_list:
    plot_bar_target(df,col)

base on above plot, we can observe these highlights:
1. EXT_SOURCE_x show differentiate of default rate amoungst bins;
2. AMT_GOODS_PRICE shows differentiate of default rate amongst bins


Now we finish on data analysis, let's use these groups and update to calculate IV and WoE in order to select meaningful feature for model.


## 2.2. Correlation & Data distribution analysis

First, let's have a quick look on heatmap correlation to identify potential issue on existing feature list.

#once finalize the list after calculate WoE, to check correlation of selected features
corr_matrix = df[num_list[2:len(num_list)]].corr()

plt.figure(figsize=(15, 13))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
    
plt.xticks(fontsize=7)  
plt.yticks(fontsize=7)
plt.title('Correlation Heatmap', fontsize=10)
plt.show()
plt.close()

during data inspection as well as above roughcheck on correlation heatmap, we can notice that there are potential multicollinearity, especially on group of normalize data. To confirm this hypothesis, let's calculate VIF on list of single numeric variable.

from statsmodels.stats.outliers_influence import variance_inflation_factor
np.seterr(divide='ignore', invalid='ignore') #ignore error

# Calculate VIF for each predictor variable, except group of DOCUMENT have been merged into FLAG_DOCUMENT_EXTRA
col = df.columns[3:95].tolist()
col.extend(['FLAG_DOCUMENT_EXTRA', 'FLAG_DOCUMENT_3'])
col.extend(df.columns[117:122].tolist()) 
vif_data = df[col].select_dtypes(include=np.number).copy()  # Ensure to work with numeric data only

for i in df.columns[117:122]:
    vif_data[i] = df[i].copy()

vif_data['Intercept'] = 1

vif = pd.DataFrame()
vif['VAR'] = vif_data.columns
vif['VIF'] = [variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])]
len(vif[vif['VIF'] >= 10]) #check number of inflation feature 

base on above result, we can see there are 50 numeric variables has VIF >= 10, confirm that there are multicollinearity in existing list features. To solve this issue, we will remove these high inflation variables from selected features.
next step, let's have a quick look on correlation of remaining features.

corr_matrix = df[vif[vif['VIF'] < 10]['VAR'].tolist()[0:-1]].corr()

plt.figure(figsize=(15, 13))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
    
plt.xticks(fontsize=7)  
plt.yticks(fontsize=7)
plt.title('Correlation Heatmap', fontsize=10)
plt.show()
plt.close()

We can observe more reasonable level of correlation amongst features.
Next step, let's calculate WoE and IV on categorical data, including _BIN of selected continuous data.

remove_cat = []
for i in cat_list:
    if len(df[i].value_counts()) <= 1:
        remove_cat.append(i)
refine_cat_list = [el for el in cat_list if el not in remove_cat]

selected_list=[]
for i in vif[vif['VIF'] < 10]['VAR'].tolist()[0:-1]:
    bin_var = i+'_BIN'
    if bin_var in bin_list:
        selected_list.append(bin_var)
    else:
        selected_list.append(i)

selected_list = selected_list + refine_cat_list
selected_list.append('ORGANIZATION_TYPE_TRANSF')

set_list = set()
for s in selected_list:
    set_list.add(s)

doc_list = set(['FLAG_DOCUMENT_11',
 'FLAG_DOCUMENT_12',
 'FLAG_DOCUMENT_13',
 'FLAG_DOCUMENT_14',
 'FLAG_DOCUMENT_15',
 'FLAG_DOCUMENT_16',
 'FLAG_DOCUMENT_17',
 'FLAG_DOCUMENT_18',
 'FLAG_DOCUMENT_19',
 'FLAG_DOCUMENT_2',
 'FLAG_DOCUMENT_20',
 'FLAG_DOCUMENT_21',
 'FLAG_DOCUMENT_4',
 'FLAG_DOCUMENT_5',
 'FLAG_DOCUMENT_6',
 'FLAG_DOCUMENT_8',
 'FLAG_DOCUMENT_9',
 'ORGANIZATION_TYPE'])

selected_list = [x for x in set_list if x not in doc_list]
selected_list.sort()
len(selected_list)

# Function to calculate WoE and IV for a single feature
def calculate_iv(df, feature, target):
    # Create a summary DataFrame to calculate IV
    summary = df.groupby(feature)[target].agg(['count', 'sum'])
    summary.columns = ['#total', '#bad']
    summary['#good'] = summary['#total'] - summary['#bad']
    summary['%bad'] = summary['#bad'] / summary['#bad'].sum()
    summary['%good'] = summary['#good'] / summary['#good'].sum()
    summary['woe'] = np.log(summary['%good'] / summary['%bad'])
    summary['iv'] = (summary['%good'] - summary['%bad']) * summary['woe']
    iv = summary['iv'].sum()
    return iv, summary

# Calculate IV for each feature in the DataFrame
iv_values = {}
summaries = {}

for feature in selected_list:
    if feature != 'TARGET':  # Exclude the target variable
        iv, summary = calculate_iv(df, feature, 'TARGET')
        iv_values[feature] = iv
        summaries[feature] = summary

# Print IV values for each feature
for feature, iv in iv_values.items():
    print(f"IV for {feature}: {iv:.4f}")

# Optionally, print summaries for each feature
for feature, summary in summaries.items():
    print(f"\nSummary for {feature}:")
    print(summary)
    

we observe some features has IV = inf, let's inspect in detail:

inf_iv = []
for i, v in iv_values.items():
    if v ==  float('inf'):
        inf_iv.append(i)
        
for i in inf_iv:
    print(pd.DataFrame(summaries[i]))


In this scope of project, since there are too many variables can impact to model robust, threshold for IV value will be set at 0.05 to only allow fair predictive features. For feature has IV < 0.05 will be remove from final_features before fitting data to model.
From above result, we can address the inf value caused from small group contain 0 data of good/bad record. Statistically, we can assume distribution of good/bad in these cases with small value 0.00000001. By that, selected feature based on correcting IV to be adding NAME_INCOME_TYPE with IV 0.06

#selecting feature has IV >= 0.1:
final_feature = []
for feature, iv in iv_values.items():
    if iv >= 0.01 and iv != float('inf'):
        final_feature.append(feature)

#update inspecting feature has IV = inf as above analysis:
final_feature.append('NAME_INCOME_TYPE')

#adding target
final_feature.append('TARGET')

df_filter = df[final_feature]

print(len(df_filter.columns))

now with this final set of 31 features, let's proceed to model fitting and selection

## 3. BUILDING MODEL

In this project, for classification problem, Logistic Regression will be used.
Dataset now will be transformed to fit in model.

X = pd.get_dummies(df_filter.drop('TARGET',axis = 1),drop_first=True)
y = df_filter['TARGET']

To assessing model prediction, dataset will be splitted into 3 parts: 50% training, 25% test and 25% validation.
With 31 features in various scale, we will also use StandardScaler to standardize the scale of feature character.

### 3.1 base model

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score


X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42) #train plit
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) #test x validation split
X_train_scaled = StandardScaler().fit_transform(X_train)
X_test_scaled = StandardScaler().fit_transform(X_test)
X_val_scaled = StandardScaler().fit_transform(X_val)

model = LogisticRegression(random_state = 42, max_iter=1000) # Instantiate the model with balanced class_weight
model.fit(X_train_scaled,y_train) # Fit the model using training set

#have a look on current params
model.get_params()


### 3.2 assessing base model:

#TEST SET
y_pred_t = model.predict(X_test_scaled)
y_pred_probs_t = model.predict_proba(X_test_scaled)[:, 1]
auc_roc_t = roc_auc_score(y_test, y_pred_probs_t)
CX_test = confusion_matrix(y_test, y_pred_t)


#VAL SET
y_pred_v = model.predict(X_val_scaled)
y_pred_probs_v = model.predict_proba(X_val_scaled)[:, 1]
auc_roc_v = roc_auc_score(y_val, y_pred_probs_v)
CX_val = confusion_matrix(y_val, y_pred_v)


#fpr_tr, tpr_tr, _ = roc_curve(y_train,  y_pred_probs_tr)
fpr_t, tpr_t, _ = roc_curve(y_test,  y_pred_probs_t)
fpr_v, tpr_v, _ = roc_curve(y_val,  y_pred_probs_v)

#plt.plot(fpr_tr,tpr_tr, label='AUC train_set:'+str(round(auc_roc_tr,5)))
plt.plot(fpr_t,tpr_t, label='AUC test_set:'+str(round(auc_roc_t,5)))
plt.plot(fpr_v,tpr_v, label='AUC val_set:'+str(round(auc_roc_v,5)))

plt.axline((0,0), slope=1,fillstyle='full', c='green', linestyle='-.', linewidth=0.5)

plt.title('AUC ROC Curve')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend()
plt.show() 

We can see the model is fitting to fair AUC score above 0.7 with describe method above. This score is above the targeting benchmark of 0.68. However, let's also try to look on tuning param by GridSearchCV with standard fold = 5, to see if we can finetuning to any better performance.

### 3.3 best model

In this step, we will tune param using GridSearchCV, target to achieve best model

param_grid = {
    'C': [0.001, 0.01, 1],  # Regularization parameter
    'solver': ['lbfgs','newton-cg', 'newton-cholesky', 'liblinear']       # Solver type
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)

print("Best parameters:", grid_search.best_params_)

# Evaluate the model with best parameters on test data
best_model = grid_search.best_estimator_
y_pred_probs_b = best_model.predict_proba(X_test_scaled)[:, 1]
auc_roc_b = roc_auc_score(y_test, y_pred_probs_b)
print("AUC:", auc_roc_b)

best_model.get_params()

### 3.4 assessing best model

#evaluation on best_param

#TEST SET
y_pred_t = best_model.predict(X_test_scaled)
y_pred_probs_t = best_model.predict_proba(X_test_scaled)[:, 1]
auc_roc_t = roc_auc_score(y_test, y_pred_probs_t)
CX_test = confusion_matrix(y_test, y_pred_t)


#VAL SET
y_pred_v = best_model.predict(X_val_scaled)
y_pred_probs_v = best_model.predict_proba(X_val_scaled)[:, 1]
auc_roc_v = roc_auc_score(y_val, y_pred_probs_v)
CX_val = confusion_matrix(y_val, y_pred_v)


#fpr_tr, tpr_tr, _ = roc_curve(y_train,  y_pred_probs_tr)
fpr_t, tpr_t, _ = roc_curve(y_test,  y_pred_probs_t)
fpr_v, tpr_v, _ = roc_curve(y_val,  y_pred_probs_v)

#plt.plot(fpr_tr,tpr_tr, label='AUC train_set:'+str(round(auc_roc_tr,5)))
plt.plot(fpr_t,tpr_t, label='AUC test_set:'+str(round(auc_roc_t,5)))
plt.plot(fpr_v,tpr_v, label='AUC val_set:'+str(round(auc_roc_v,5)))

plt.axline((0,0), slope=1,fillstyle='full', c='green', linestyle='-.', linewidth=0.5)

plt.title('AUC ROC Curve')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend()
plt.show() 

## 4. MODEL EVALUATION

After param tuning, we observe no significant different between the base model using default value vs. tuning model using best_param. Therefore, we can interchangeably these 2 models. 

This result likely happened when we did select proper set of feature through intensive analysis, careful feature engineering & feature scaling earlier, so that it contributes to learning progress initially, therefore the tuning param can help little on improving performance in Logistic Regression in binary class prediction.

To demonstrate the next analysis on predictive result using confusion matrix, as showing slightly improve comparing to base model, evaluation on result of best_model will be used.

cm_clf1 = CX_test
cm_clf2 = CX_val

# Define class labels
classes = ['good', 'bad']

# Plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns

# Plot confusion matrix for classifier 1
axs[0].imshow(cm_clf1, cmap='Blues', interpolation='nearest')
axs[0].set(xticks=np.arange(cm_clf1.shape[1]),
           yticks=np.arange(cm_clf1.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title='Confusion Matrix - test set',
           xlabel='Predicted label',
           ylabel='True label')
plt.setp(axs[0].get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")
for i in range(cm_clf1.shape[0]):
    for j in range(cm_clf1.shape[1]):
        axs[0].text(j, i, format(cm_clf1[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm_clf1[i, j] > cm_clf1.max() / 2. else "black")

# Plot confusion matrix for classifier 2
axs[1].imshow(cm_clf2, cmap='Greens', interpolation='nearest')
axs[1].set(xticks=np.arange(cm_clf2.shape[1]),
           yticks=np.arange(cm_clf2.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title='Confusion Matrix - Validation set',
           xlabel='Predicted label',
           ylabel='True label')
plt.setp(axs[1].get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")
for i in range(cm_clf2.shape[0]):
    for j in range(cm_clf2.shape[1]):
        axs[1].text(j, i, format(cm_clf2[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm_clf2[i, j] > cm_clf2.max() / 2. else "black")

plt.tight_layout()
plt.show()


  let's have a quick analysis on test set & validation set to see how the model 
    
    1. TEST SET:
       - original default rate = true_bad / total sample = (5693 + 55) / 69558 = 0.08264
       - default rate by best_model = (true_bad|predicted_good) / predict_good = 5693 / (5693 + 63748) = 0.08198

    2. VALIDATION SET:
       - original default rate = true_bad / total sample = (5622 + 51) / 69558 = 0.08156
       - default rate by best_model = (true_bad|predicted_good) / predict_good = 5622 / (5622 + 63819) = 0.08096

   Above result proved that using model can help to improve default rate ~0.06%.


<font face="verdana" color="purple" size = 5>
<br>
CONCLUSION
</font>

<font face="verdana" color="purple" size = 2>
Key take away from this project:
    <br>
    1. Data cleaning, method of feature selection and feature engineering play important role in build a good model.
    <br>
    2. Using model prediction can help to reduce ~0.06% default rate in this project scope.
    <br>
    3. Recommend to also considering the increase in rejection rate when using model to predict, compare potential loss of interest rate from rejected incorrect predicted bad customer vs. saving loss from correct predicted bad customer, to decide model release to production.
</font>
